{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29057e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal\n",
    "import pandas as pd\n",
    "from scipy.stats import truncnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6770bd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((872, 5), (872,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/data/bank-note/train.csv\", header=None)\n",
    "d = df.to_numpy()\n",
    "X = d[:,:-1]\n",
    "x = np.hstack((X,np.ones((X.shape[0],1))))\n",
    "y = d[:,-1] \n",
    "# y = np.expand_dims(d[:,-1], axis=-1) \n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f013c7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 5), (500,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/data/bank-note/test.csv\", header=None)\n",
    "d = df.to_numpy()\n",
    "x_test = d[:,:-1]\n",
    "x_test = np.hstack((x_test,np.ones((x_test.shape[0],1))))\n",
    "\n",
    "y_test = d[:,-1]\n",
    "# y_test = np.expand_dims(d[:,-1], axis=-1) \n",
    "\n",
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e334ca",
   "metadata": {},
   "source": [
    "# Part a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61545800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(weight, data, regression= \"logistic\"):\n",
    "    \"\"\"\n",
    "        Output of different regression. Taken from Assignment 2.\n",
    "        returns #examples x 1 arrays\n",
    "    \"\"\"\n",
    "    dot_product = np.matmul(data,weight)\n",
    "    if regression == \"logistic\":\n",
    "        output = get_sigmoid(dot_product)\n",
    "    elif regression == \"probit\":\n",
    "        output = norm.cdf(dot_product)\n",
    "    elif regression == \"multiclass\":\n",
    "        output = softmax(dot_product, axis=1)\n",
    "\n",
    "    return output, dot_product\n",
    "\n",
    "def get_log_likelihood(phi, pred, t, dot_product, weight, reg= 1):\n",
    "    \"\"\"\n",
    "        Returns log likelihood of the logistic regression\n",
    "        t = N x 1\n",
    "    \"\"\"\n",
    "    prior = -0.5* np.sum(np.multiply(weight, weight))\n",
    "    likelihood = np.multiply(t, np.log(pred+TOLERANCE)) + np.multiply(1.0- t, np.log(1.0-pred+TOLERANCE))\n",
    "    likelihood = np.sum(likelihood)\n",
    "\n",
    "    return prior + likelihood\n",
    "\n",
    "def get_sigmoid(x):\n",
    "    \"\"\"\n",
    "        Numerically stable version of sigmoid function. Taken from Assignment 2.\n",
    "    \"\"\"  \n",
    "    output = np.zeros(x.shape)\n",
    "    ind1 = (x >= 0)\n",
    "    ind2 = (x  < 0)\n",
    "    output[ind1] = 1 / (1 + np.exp(-x[ind1]))\n",
    "    output[ind2] = np.divide(np.exp(x[ind2]), (1 + np.exp(x[ind2])))\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_gradient(phi, pred, t, dot_product, weight, reg= 1, regression= \"logistic\"):\n",
    "    \"\"\"\n",
    "        Returns log likelihood of the logistic regression. Taken from Assignment 2\n",
    "        t = (N, 1)\n",
    "        weight = (D, 1)\n",
    "    \"\"\"\n",
    "    if regression == \"logistic\":\n",
    "        gradient = np.matmul(phi.T, pred - t)\n",
    "    elif regression == \"probit\":\n",
    "        R = np.eye(pred.shape[0])\n",
    "        for i in range(pred.shape[0]):\n",
    "            y_n  = pred[i,0]\n",
    "            dotp = dot_product[i, 0]\n",
    "            pdf  = norm.pdf(dotp)\n",
    "            R[i,i] = pdf/(y_n*(1-y_n) + TOLERANCE)\n",
    "        gradient = np.matmul(np.matmul(phi.T, R), pred-t)\n",
    "    elif regression == \"multiclass\":\n",
    "        gradient = np.matmul(phi.T, pred - t)\n",
    "\n",
    "    # Add regularization\n",
    "    gradient += weight/ reg\n",
    "    return gradient\n",
    "\n",
    "def get_KE(p, scale= 1):\n",
    "    \"\"\" \n",
    "        Returns KE from the momentum vector\n",
    "    \"\"\"\n",
    "    p = p.flatten()\n",
    "    return scale * 0.5*np.sum(np.multiply(p, p))\n",
    "\n",
    "def to_accept_without_log(x, x_new):\n",
    "    \"\"\"\n",
    "        Acceptance rule without any log. \n",
    "    \"\"\"\n",
    "    if x_new>x:\n",
    "        return True\n",
    "    else:\n",
    "        accept=np.random.uniform(0,1)\n",
    "        return (accept < x_new/(x+TOLERANCE))\n",
    "\n",
    "def hybrid_monte_carlo(train_data, train_label, z_init, num_iterations, epsilon, num_leapfrog_steps, collect_final_sample_frequency= 10, display_frequency= 5000, scale_KE= 1):\n",
    "    \"\"\"\n",
    "        Gets posterior samples for Bayes Logistic Regression using HMC algorithm\n",
    "        z_int= (dim, 1)\n",
    "    \"\"\"\n",
    "    dim = train_data.shape[1]\n",
    "    z = z_init\n",
    "\n",
    "    accepted = [] # Keeps track of accepted samples\n",
    "    sampled  = [] # Keeps track of all samples\n",
    "    final    = [] # Keeps track of final samples which are sampled in a cyclic manner\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Old energy = -loglik and Old gradient\n",
    "        pred, dot_product = get_output(z, train_data)\n",
    "        old_PE   =  -get_log_likelihood(phi= train_data, pred= pred, t= train_label[:, np.newaxis], dot_product= dot_product, weight= z)\n",
    "        \n",
    "        # There is no minus since gradient function returns gradient of negative log likelihood\n",
    "        old_grad =  get_gradient(phi= train_data, pred= pred, t= train_label[:, np.newaxis], dot_product= dot_product, weight= z)\n",
    "\n",
    "        new_z = np.copy(z)              # deep copy of array\n",
    "        new_grad  = np.copy(old_grad)   # deep copy of array\n",
    "\n",
    "        # draw random momentum vector from unit Gaussian which decides the energy\n",
    "        # given out for exploration\n",
    "        p = np.random.normal(0.0, 1.0, (dim, 1))\n",
    "\n",
    "        # Compute Hamiltonian\n",
    "        H = get_KE(p, scale= scale_KE) + old_PE\n",
    "\n",
    "        # Suggest new candidate using gradient + Hamiltonian dynamics.\n",
    "        # Leapfrog\n",
    "        for j in range(num_leapfrog_steps):  \n",
    "            # Make first half step in p, full step in z and then again half step in p\n",
    "            p        -= (epsilon/2.0)*new_grad\n",
    "            new_z    += epsilon*p\n",
    "            pred, dot_product = get_output(new_z, train_data)\n",
    "            new_grad  = get_gradient(phi= train_data, pred= pred, t= train_label[:, np.newaxis], dot_product= dot_product, weight= new_z)\n",
    "            p        -= (epsilon/2.0)*new_grad\n",
    "\n",
    "        # Compute new Hamiltonian\n",
    "        pred, dot_product = get_output(new_z, train_data)\n",
    "        new_PE = -get_log_likelihood(phi= train_data, pred= pred, t= train_label[:, np.newaxis], dot_product= dot_product, weight= new_z)\n",
    "        new_H  = get_KE(p, scale= scale_KE) + new_PE\n",
    "        \n",
    "        sampled.append(new_z)\n",
    "        \n",
    "        # Accept new candidate in Monte-Carlo fashion.\n",
    "        if to_accept_without_log(get_prob_from_energy(H), get_prob_from_energy(new_H)):            \n",
    "            z = new_z\n",
    "            accepted.append(new_z)\n",
    "\n",
    "        if i % collect_final_sample_frequency == 0:\n",
    "            # Sample from the current parameters\n",
    "            final.append(z)\n",
    "\n",
    "        if (i+1) % display_frequency == 0 or i == num_iterations-1:\n",
    "            print(\"Iter {:6d} done\".format(i+1))\n",
    "    \n",
    "    return np.array(accepted), np.array(sampled), np.array(final), z\n",
    "\n",
    "def get_prob_from_energy(energy):\n",
    "    return np.exp(-energy)\n",
    "\n",
    "def get_accuracy(pred, test_label, regression= \"logistic\"):\n",
    "    \"\"\"\n",
    "        Gets accuracy in % for predictions. Taken from Assignment 2.\n",
    "    \"\"\"\n",
    "    if regression == \"multiclass\":\n",
    "        pred_max = np.argmax(pred, axis=1)\n",
    "        gt_max   = np.argmax(test_label, axis=1)\n",
    "        acc = np.sum(pred_max == gt_max)*100.0/pred.shape[0]\n",
    "    elif regression == \"logistic\" or regression == \"probit\":\n",
    "        if pred.ndim == 2:\n",
    "            pred = pred[:,0]\n",
    "        pred[pred >= 0.5] = 1.0\n",
    "        pred[pred <  0.5] = 0.0\n",
    "    acc = np.sum(pred == test_label)*100.0/pred.shape[0]\n",
    "\n",
    "    return acc\n",
    "\n",
    "def get_prediction_likelihood_without_complications(test_data, test_label, weight):\n",
    "    \"\"\"\n",
    "        Returns prediction likelihood on a sample weight without using any hessian\n",
    "        test_data  = N x D\n",
    "        test_label = N \n",
    "        weight     = D x 1\n",
    "    \"\"\"\n",
    "    pred, _ = get_output(weight, test_data)\n",
    "    pred = pred[:,0]\n",
    "    pred_like = np.multiply(test_label, np.log(pred + TOLERANCE)) + np.multiply(1.0-test_label, np.log(1.0-pred+ TOLERANCE))\n",
    "    return np.exp(np.mean(pred_like))\n",
    "\n",
    "def test_on_posterior(test_data, test_label, posterior_samples):\n",
    "    \"\"\"\n",
    "        Returns stats on posterior samples\n",
    "    \"\"\"\n",
    "    print(\"Testing on posterior samples...\")\n",
    "    num_posterior_samples = posterior_samples.shape[0]\n",
    "    avg_pred_test    = np.zeros((num_posterior_samples, ))\n",
    "    avg_pred_log_lld = np.zeros((num_posterior_samples, ))\n",
    "                    \n",
    "    for k in range(num_posterior_samples):\n",
    "        # Use the posterior samples\n",
    "        w_sampled = posterior_samples[k]\n",
    "        \n",
    "        # Get the hessian\n",
    "        #pred, dot_product = get_output(w_sampled, train_data)\n",
    "        #hessian  = get_hessian (phi= train_data, pred= pred[:, np.newaxis], t= train_label[:, np.newaxis], dot_product= dot_product)\n",
    "        \n",
    "        pred_test, _         = get_output  (w_sampled, test_data)\n",
    "        acc                  = get_accuracy(pred_test, test_label) \n",
    "        pred_likelihood      = get_prediction_likelihood_without_complications(test_data, test_label, w_sampled) #get_prediction_likelihood(test_data, test_label, w_sampled, hessian)\n",
    "        avg_pred_test[k]     = acc\n",
    "        avg_pred_log_lld [k] = np.log(pred_likelihood)\n",
    "        \n",
    "        if (k+1)%100 == 0 or k== num_posterior_samples-1:\n",
    "            print(\"{:5d} Posterior Weight samples Test_data Pred_acc= {:.2f}, Pred_log_likelihood= {:.2f}\".format(k+1, np.mean(avg_pred_test[:k]), np.mean(avg_pred_log_lld[:k])))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a736fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================================================\n",
      "\tHamiltonian Monte Carlo Sampling with Leapfrog\n",
      "=======================================================================\n",
      "\n",
      "Burnin stage, epsilon = 0.005, L= 10\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Iter  15000 done\n",
      "Iter  20000 done\n",
      "Iter  25000 done\n",
      "Iter  30000 done\n",
      "Iter  35000 done\n",
      "Iter  40000 done\n",
      "Iter  45000 done\n",
      "Iter  50000 done\n",
      "Iter  55000 done\n",
      "Iter  60000 done\n",
      "Iter  65000 done\n",
      "Iter  70000 done\n",
      "Iter  75000 done\n",
      "Iter  80000 done\n",
      "Iter  85000 done\n",
      "Iter  90000 done\n",
      "Iter  95000 done\n",
      "Iter 100000 done\n",
      "Generating samples after burnin stage...\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Testing on posterior samples...\n",
      "  100 Posterior Weight samples Test_data Pred_acc= 99.00, Pred_log_likelihood= -0.04\n",
      "  200 Posterior Weight samples Test_data Pred_acc= 99.00, Pred_log_likelihood= -0.04\n",
      "  300 Posterior Weight samples Test_data Pred_acc= 99.00, Pred_log_likelihood= -0.04\n",
      "  400 Posterior Weight samples Test_data Pred_acc= 99.00, Pred_log_likelihood= -0.04\n",
      "  500 Posterior Weight samples Test_data Pred_acc= 99.00, Pred_log_likelihood= -0.04\n",
      "  600 Posterior Weight samples Test_data Pred_acc= 99.00, Pred_log_likelihood= -0.04\n",
      "  700 Posterior Weight samples Test_data Pred_acc= 99.00, Pred_log_likelihood= -0.04\n",
      "  800 Posterior Weight samples Test_data Pred_acc= 99.00, Pred_log_likelihood= -0.04\n",
      "  900 Posterior Weight samples Test_data Pred_acc= 99.00, Pred_log_likelihood= -0.04\n",
      " 1000 Posterior Weight samples Test_data Pred_acc= 99.00, Pred_log_likelihood= -0.04\n",
      "Acceptance rate= 0.001400\n",
      "\n",
      "Burnin stage, epsilon = 0.005, L= 20\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Iter  15000 done\n",
      "Iter  20000 done\n",
      "Iter  25000 done\n",
      "Iter  30000 done\n",
      "Iter  35000 done\n",
      "Iter  40000 done\n",
      "Iter  45000 done\n",
      "Iter  50000 done\n",
      "Iter  55000 done\n",
      "Iter  60000 done\n",
      "Iter  65000 done\n",
      "Iter  70000 done\n",
      "Iter  75000 done\n",
      "Iter  80000 done\n",
      "Iter  85000 done\n",
      "Iter  90000 done\n",
      "Iter  95000 done\n",
      "Iter 100000 done\n",
      "Generating samples after burnin stage...\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Testing on posterior samples...\n",
      "  100 Posterior Weight samples Test_data Pred_acc= 99.02, Pred_log_likelihood= -0.03\n",
      "  200 Posterior Weight samples Test_data Pred_acc= 99.01, Pred_log_likelihood= -0.04\n",
      "  300 Posterior Weight samples Test_data Pred_acc= 99.01, Pred_log_likelihood= -0.04\n",
      "  400 Posterior Weight samples Test_data Pred_acc= 99.01, Pred_log_likelihood= -0.04\n",
      "  500 Posterior Weight samples Test_data Pred_acc= 99.01, Pred_log_likelihood= -0.04\n",
      "  600 Posterior Weight samples Test_data Pred_acc= 99.01, Pred_log_likelihood= -0.04\n",
      "  700 Posterior Weight samples Test_data Pred_acc= 99.01, Pred_log_likelihood= -0.04\n",
      "  800 Posterior Weight samples Test_data Pred_acc= 99.00, Pred_log_likelihood= -0.03\n",
      "  900 Posterior Weight samples Test_data Pred_acc= 99.00, Pred_log_likelihood= -0.03\n",
      " 1000 Posterior Weight samples Test_data Pred_acc= 99.00, Pred_log_likelihood= -0.03\n",
      "Acceptance rate= 0.050300\n",
      "\n",
      "Burnin stage, epsilon = 0.005, L= 50\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Iter  15000 done\n",
      "Iter  20000 done\n",
      "Iter  25000 done\n",
      "Iter  30000 done\n",
      "Iter  35000 done\n",
      "Iter  40000 done\n",
      "Iter  45000 done\n",
      "Iter  50000 done\n",
      "Iter  55000 done\n",
      "Iter  60000 done\n",
      "Iter  65000 done\n",
      "Iter  70000 done\n",
      "Iter  75000 done\n",
      "Iter  80000 done\n",
      "Iter  85000 done\n",
      "Iter  90000 done\n",
      "Iter  95000 done\n",
      "Iter 100000 done\n",
      "Generating samples after burnin stage...\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Testing on posterior samples...\n",
      "  100 Posterior Weight samples Test_data Pred_acc= 96.60, Pred_log_likelihood= -0.09\n",
      "  200 Posterior Weight samples Test_data Pred_acc= 96.60, Pred_log_likelihood= -0.09\n",
      "  300 Posterior Weight samples Test_data Pred_acc= 96.60, Pred_log_likelihood= -0.09\n",
      "  400 Posterior Weight samples Test_data Pred_acc= 96.60, Pred_log_likelihood= -0.09\n",
      "  500 Posterior Weight samples Test_data Pred_acc= 96.60, Pred_log_likelihood= -0.09\n",
      "  600 Posterior Weight samples Test_data Pred_acc= 96.60, Pred_log_likelihood= -0.09\n",
      "  700 Posterior Weight samples Test_data Pred_acc= 96.60, Pred_log_likelihood= -0.09\n",
      "  800 Posterior Weight samples Test_data Pred_acc= 96.60, Pred_log_likelihood= -0.09\n",
      "  900 Posterior Weight samples Test_data Pred_acc= 96.60, Pred_log_likelihood= -0.09\n",
      " 1000 Posterior Weight samples Test_data Pred_acc= 96.60, Pred_log_likelihood= -0.09\n",
      "Acceptance rate= 0.000000\n",
      "\n",
      "Burnin stage, epsilon = 0.010, L= 10\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Iter  15000 done\n",
      "Iter  20000 done\n",
      "Iter  25000 done\n",
      "Iter  30000 done\n",
      "Iter  35000 done\n",
      "Iter  40000 done\n",
      "Iter  45000 done\n",
      "Iter  50000 done\n",
      "Iter  55000 done\n",
      "Iter  60000 done\n",
      "Iter  65000 done\n",
      "Iter  70000 done\n",
      "Iter  75000 done\n",
      "Iter  80000 done\n",
      "Iter  85000 done\n",
      "Iter  90000 done\n",
      "Iter  95000 done\n",
      "Iter 100000 done\n",
      "Generating samples after burnin stage...\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Testing on posterior samples...\n",
      "  100 Posterior Weight samples Test_data Pred_acc= 99.06, Pred_log_likelihood= -0.03\n",
      "  200 Posterior Weight samples Test_data Pred_acc= 99.03, Pred_log_likelihood= -0.03\n",
      "  300 Posterior Weight samples Test_data Pred_acc= 99.02, Pred_log_likelihood= -0.03\n",
      "  400 Posterior Weight samples Test_data Pred_acc= 99.03, Pred_log_likelihood= -0.03\n",
      "  500 Posterior Weight samples Test_data Pred_acc= 99.02, Pred_log_likelihood= -0.03\n",
      "  600 Posterior Weight samples Test_data Pred_acc= 99.02, Pred_log_likelihood= -0.03\n",
      "  700 Posterior Weight samples Test_data Pred_acc= 99.02, Pred_log_likelihood= -0.03\n",
      "  800 Posterior Weight samples Test_data Pred_acc= 99.01, Pred_log_likelihood= -0.03\n",
      "  900 Posterior Weight samples Test_data Pred_acc= 99.01, Pred_log_likelihood= -0.03\n",
      " 1000 Posterior Weight samples Test_data Pred_acc= 99.00, Pred_log_likelihood= -0.03\n",
      "Acceptance rate= 0.041400\n",
      "\n",
      "Burnin stage, epsilon = 0.010, L= 20\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Iter  15000 done\n",
      "Iter  20000 done\n",
      "Iter  25000 done\n",
      "Iter  30000 done\n",
      "Iter  35000 done\n",
      "Iter  40000 done\n",
      "Iter  45000 done\n",
      "Iter  50000 done\n",
      "Iter  55000 done\n",
      "Iter  60000 done\n",
      "Iter  65000 done\n",
      "Iter  70000 done\n",
      "Iter  75000 done\n",
      "Iter  80000 done\n",
      "Iter  85000 done\n",
      "Iter  90000 done\n",
      "Iter  95000 done\n",
      "Iter 100000 done\n",
      "Generating samples after burnin stage...\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Testing on posterior samples...\n",
      "  100 Posterior Weight samples Test_data Pred_acc= 99.02, Pred_log_likelihood= -0.03\n",
      "  200 Posterior Weight samples Test_data Pred_acc= 99.01, Pred_log_likelihood= -0.03\n",
      "  300 Posterior Weight samples Test_data Pred_acc= 99.00, Pred_log_likelihood= -0.03\n",
      "  400 Posterior Weight samples Test_data Pred_acc= 98.99, Pred_log_likelihood= -0.03\n",
      "  500 Posterior Weight samples Test_data Pred_acc= 98.99, Pred_log_likelihood= -0.03\n",
      "  600 Posterior Weight samples Test_data Pred_acc= 98.99, Pred_log_likelihood= -0.03\n",
      "  700 Posterior Weight samples Test_data Pred_acc= 98.97, Pred_log_likelihood= -0.03\n",
      "  800 Posterior Weight samples Test_data Pred_acc= 98.97, Pred_log_likelihood= -0.03\n",
      "  900 Posterior Weight samples Test_data Pred_acc= 98.97, Pred_log_likelihood= -0.03\n",
      " 1000 Posterior Weight samples Test_data Pred_acc= 98.97, Pred_log_likelihood= -0.03\n",
      "Acceptance rate= 0.126800\n",
      "\n",
      "Burnin stage, epsilon = 0.010, L= 50\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Iter  15000 done\n",
      "Iter  20000 done\n",
      "Iter  25000 done\n",
      "Iter  30000 done\n",
      "Iter  35000 done\n",
      "Iter  40000 done\n",
      "Iter  45000 done\n",
      "Iter  50000 done\n",
      "Iter  55000 done\n",
      "Iter  60000 done\n",
      "Iter  65000 done\n",
      "Iter  70000 done\n",
      "Iter  75000 done\n",
      "Iter  80000 done\n",
      "Iter  85000 done\n",
      "Iter  90000 done\n",
      "Iter  95000 done\n",
      "Iter 100000 done\n",
      "Generating samples after burnin stage...\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Testing on posterior samples...\n",
      "  100 Posterior Weight samples Test_data Pred_acc= 98.98, Pred_log_likelihood= -0.03\n",
      "  200 Posterior Weight samples Test_data Pred_acc= 98.99, Pred_log_likelihood= -0.03\n",
      "  300 Posterior Weight samples Test_data Pred_acc= 98.99, Pred_log_likelihood= -0.03\n",
      "  400 Posterior Weight samples Test_data Pred_acc= 98.97, Pred_log_likelihood= -0.03\n",
      "  500 Posterior Weight samples Test_data Pred_acc= 98.98, Pred_log_likelihood= -0.03\n",
      "  600 Posterior Weight samples Test_data Pred_acc= 98.97, Pred_log_likelihood= -0.03\n",
      "  700 Posterior Weight samples Test_data Pred_acc= 98.98, Pred_log_likelihood= -0.03\n",
      "  800 Posterior Weight samples Test_data Pred_acc= 98.98, Pred_log_likelihood= -0.03\n",
      "  900 Posterior Weight samples Test_data Pred_acc= 98.98, Pred_log_likelihood= -0.03\n",
      " 1000 Posterior Weight samples Test_data Pred_acc= 98.98, Pred_log_likelihood= -0.03\n",
      "Acceptance rate= 0.087400\n",
      "\n",
      "Burnin stage, epsilon = 0.020, L= 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Iter  15000 done\n",
      "Iter  20000 done\n",
      "Iter  25000 done\n",
      "Iter  30000 done\n",
      "Iter  35000 done\n",
      "Iter  40000 done\n",
      "Iter  45000 done\n",
      "Iter  50000 done\n",
      "Iter  55000 done\n",
      "Iter  60000 done\n",
      "Iter  65000 done\n",
      "Iter  70000 done\n",
      "Iter  75000 done\n",
      "Iter  80000 done\n",
      "Iter  85000 done\n",
      "Iter  90000 done\n",
      "Iter  95000 done\n",
      "Iter 100000 done\n",
      "Generating samples after burnin stage...\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Testing on posterior samples...\n",
      "  100 Posterior Weight samples Test_data Pred_acc= 98.95, Pred_log_likelihood= -0.03\n",
      "  200 Posterior Weight samples Test_data Pred_acc= 98.95, Pred_log_likelihood= -0.03\n",
      "  300 Posterior Weight samples Test_data Pred_acc= 98.95, Pred_log_likelihood= -0.03\n",
      "  400 Posterior Weight samples Test_data Pred_acc= 98.96, Pred_log_likelihood= -0.03\n",
      "  500 Posterior Weight samples Test_data Pred_acc= 98.96, Pred_log_likelihood= -0.03\n",
      "  600 Posterior Weight samples Test_data Pred_acc= 98.96, Pred_log_likelihood= -0.03\n",
      "  700 Posterior Weight samples Test_data Pred_acc= 98.96, Pred_log_likelihood= -0.03\n",
      "  800 Posterior Weight samples Test_data Pred_acc= 98.95, Pred_log_likelihood= -0.03\n",
      "  900 Posterior Weight samples Test_data Pred_acc= 98.95, Pred_log_likelihood= -0.03\n",
      " 1000 Posterior Weight samples Test_data Pred_acc= 98.95, Pred_log_likelihood= -0.03\n",
      "Acceptance rate= 0.126500\n",
      "\n",
      "Burnin stage, epsilon = 0.020, L= 20\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Iter  15000 done\n",
      "Iter  20000 done\n",
      "Iter  25000 done\n",
      "Iter  30000 done\n",
      "Iter  35000 done\n",
      "Iter  40000 done\n",
      "Iter  45000 done\n",
      "Iter  50000 done\n",
      "Iter  55000 done\n",
      "Iter  60000 done\n",
      "Iter  65000 done\n",
      "Iter  70000 done\n",
      "Iter  75000 done\n",
      "Iter  80000 done\n",
      "Iter  85000 done\n",
      "Iter  90000 done\n",
      "Iter  95000 done\n",
      "Iter 100000 done\n",
      "Generating samples after burnin stage...\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Testing on posterior samples...\n",
      "  100 Posterior Weight samples Test_data Pred_acc= 99.02, Pred_log_likelihood= -0.03\n",
      "  200 Posterior Weight samples Test_data Pred_acc= 99.03, Pred_log_likelihood= -0.03\n",
      "  300 Posterior Weight samples Test_data Pred_acc= 99.00, Pred_log_likelihood= -0.03\n",
      "  400 Posterior Weight samples Test_data Pred_acc= 98.99, Pred_log_likelihood= -0.03\n",
      "  500 Posterior Weight samples Test_data Pred_acc= 98.99, Pred_log_likelihood= -0.03\n",
      "  600 Posterior Weight samples Test_data Pred_acc= 99.00, Pred_log_likelihood= -0.03\n",
      "  700 Posterior Weight samples Test_data Pred_acc= 98.99, Pred_log_likelihood= -0.03\n",
      "  800 Posterior Weight samples Test_data Pred_acc= 98.99, Pred_log_likelihood= -0.03\n",
      "  900 Posterior Weight samples Test_data Pred_acc= 98.99, Pred_log_likelihood= -0.03\n",
      " 1000 Posterior Weight samples Test_data Pred_acc= 98.98, Pred_log_likelihood= -0.03\n",
      "Acceptance rate= 0.078500\n",
      "\n",
      "Burnin stage, epsilon = 0.020, L= 50\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Iter  15000 done\n",
      "Iter  20000 done\n",
      "Iter  25000 done\n",
      "Iter  30000 done\n",
      "Iter  35000 done\n",
      "Iter  40000 done\n",
      "Iter  45000 done\n",
      "Iter  50000 done\n",
      "Iter  55000 done\n",
      "Iter  60000 done\n",
      "Iter  65000 done\n",
      "Iter  70000 done\n",
      "Iter  75000 done\n",
      "Iter  80000 done\n",
      "Iter  85000 done\n",
      "Iter  90000 done\n",
      "Iter  95000 done\n",
      "Iter 100000 done\n",
      "Generating samples after burnin stage...\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Testing on posterior samples...\n",
      "  100 Posterior Weight samples Test_data Pred_acc= 98.99, Pred_log_likelihood= -0.03\n",
      "  200 Posterior Weight samples Test_data Pred_acc= 98.98, Pred_log_likelihood= -0.03\n",
      "  300 Posterior Weight samples Test_data Pred_acc= 98.98, Pred_log_likelihood= -0.03\n",
      "  400 Posterior Weight samples Test_data Pred_acc= 98.96, Pred_log_likelihood= -0.03\n",
      "  500 Posterior Weight samples Test_data Pred_acc= 98.97, Pred_log_likelihood= -0.03\n",
      "  600 Posterior Weight samples Test_data Pred_acc= 98.96, Pred_log_likelihood= -0.03\n",
      "  700 Posterior Weight samples Test_data Pred_acc= 98.96, Pred_log_likelihood= -0.03\n",
      "  800 Posterior Weight samples Test_data Pred_acc= 98.96, Pred_log_likelihood= -0.03\n",
      "  900 Posterior Weight samples Test_data Pred_acc= 98.96, Pred_log_likelihood= -0.03\n",
      " 1000 Posterior Weight samples Test_data Pred_acc= 98.97, Pred_log_likelihood= -0.03\n",
      "Acceptance rate= 0.106600\n",
      "\n",
      "Burnin stage, epsilon = 0.050, L= 10\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Iter  15000 done\n",
      "Iter  20000 done\n",
      "Iter  25000 done\n",
      "Iter  30000 done\n",
      "Iter  35000 done\n",
      "Iter  40000 done\n",
      "Iter  45000 done\n",
      "Iter  50000 done\n",
      "Iter  55000 done\n",
      "Iter  60000 done\n",
      "Iter  65000 done\n",
      "Iter  70000 done\n",
      "Iter  75000 done\n",
      "Iter  80000 done\n",
      "Iter  85000 done\n",
      "Iter  90000 done\n",
      "Iter  95000 done\n",
      "Iter 100000 done\n",
      "Generating samples after burnin stage...\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Testing on posterior samples...\n",
      "  100 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  200 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  300 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  400 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  500 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  600 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  700 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  800 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  900 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      " 1000 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "Acceptance rate= 0.000000\n",
      "\n",
      "Burnin stage, epsilon = 0.050, L= 20\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Iter  15000 done\n",
      "Iter  20000 done\n",
      "Iter  25000 done\n",
      "Iter  30000 done\n",
      "Iter  35000 done\n",
      "Iter  40000 done\n",
      "Iter  45000 done\n",
      "Iter  50000 done\n",
      "Iter  55000 done\n",
      "Iter  60000 done\n",
      "Iter  65000 done\n",
      "Iter  70000 done\n",
      "Iter  75000 done\n",
      "Iter  80000 done\n",
      "Iter  85000 done\n",
      "Iter  90000 done\n",
      "Iter  95000 done\n",
      "Iter 100000 done\n",
      "Generating samples after burnin stage...\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Testing on posterior samples...\n",
      "  100 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  200 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  300 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  400 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  500 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  600 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  700 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  800 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  900 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      " 1000 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "Acceptance rate= 0.000000\n",
      "\n",
      "Burnin stage, epsilon = 0.050, L= 50\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Iter  15000 done\n",
      "Iter  20000 done\n",
      "Iter  25000 done\n",
      "Iter  30000 done\n",
      "Iter  35000 done\n",
      "Iter  40000 done\n",
      "Iter  45000 done\n",
      "Iter  50000 done\n",
      "Iter  55000 done\n",
      "Iter  60000 done\n",
      "Iter  65000 done\n",
      "Iter  70000 done\n",
      "Iter  75000 done\n",
      "Iter  80000 done\n",
      "Iter  85000 done\n",
      "Iter  90000 done\n",
      "Iter  95000 done\n",
      "Iter 100000 done\n",
      "Generating samples after burnin stage...\n",
      "Iter   5000 done\n",
      "Iter  10000 done\n",
      "Testing on posterior samples...\n",
      "  100 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  200 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  300 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  400 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  500 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  600 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  700 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  800 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "  900 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      " 1000 Posterior Weight samples Test_data Pred_acc= 44.20, Pred_log_likelihood= -0.69\n",
      "Acceptance rate= 0.000000\n"
     ]
    }
   ],
   "source": [
    "dim = x.shape[1]\n",
    "\n",
    "num_iterations       = 100000#//10\n",
    "num_iterations_final = 10000 #//10\n",
    "collect_final_sample_frequency = 10\n",
    "display_frequency    = 5000\n",
    "TOLERANCE = 1e-5\n",
    "\n",
    "print(\"\\n=======================================================================\")\n",
    "print(\"\\tHamiltonian Monte Carlo Sampling with Leapfrog\")\n",
    "print(\"=======================================================================\")\n",
    "epsilon_array = np.array([0.005, 0.01, 0.02, 0.05])\n",
    "num_leapfrog_steps_array = np.array([10, 20, 50])\n",
    "\n",
    "for i in range(epsilon_array.shape[0]):\n",
    "    for j in range(num_leapfrog_steps_array.shape[0]):\n",
    "        epsilon            = epsilon_array[i]\n",
    "        num_leapfrog_steps = num_leapfrog_steps_array[j]\n",
    "        print(\"\\nBurnin stage, epsilon = {:.3f}, L= {}\".format(epsilon, num_leapfrog_steps))\n",
    "        w_init = np.zeros((dim, 1))\n",
    "        _, _, _, w_new = hybrid_monte_carlo(x, y, z_init= w_init, num_iterations= num_iterations, \n",
    "                                            epsilon= epsilon, num_leapfrog_steps= num_leapfrog_steps, \n",
    "                                            collect_final_sample_frequency= collect_final_sample_frequency, \n",
    "                                            scale_KE= 1)\n",
    "\n",
    "        # Remember to initialize from new values\n",
    "        print(\"Generating samples after burnin stage...\")\n",
    "        accepted, sampled, posterior_samples, _ = hybrid_monte_carlo(x, y, z_init= w_new , \n",
    "                                                                     num_iterations= num_iterations_final, \n",
    "                                                                     epsilon= epsilon, \n",
    "                                                                     num_leapfrog_steps= num_leapfrog_steps, \n",
    "                                                                     collect_final_sample_frequency= collect_final_sample_frequency,\n",
    "                                                                     scale_KE= 1)\n",
    "        acceptance_rate = accepted.shape[0]/sampled.shape[0]\n",
    "        test_on_posterior(x_test, y_test, posterior_samples)\n",
    "        print(\"Acceptance rate= {:2f}\".format(acceptance_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6933b5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9b6651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
