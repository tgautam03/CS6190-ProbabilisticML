{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9848d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import expit, logit\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39809d3",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "89e7584e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((872, 4), (872,))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"bank-note/bank-note/train.csv\", header=None)\n",
    "d = df.to_numpy()\n",
    "X = d[:,:-1]\n",
    "Y = d[:,-1]\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e4a510ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 4), (500,))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"bank-note/bank-note/test.csv\", header=None)\n",
    "d = df.to_numpy()\n",
    "Xtest = d[:,:-1]\n",
    "Ytest = d[:,-1]\n",
    "\n",
    "Xtest.shape, Ytest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac277957",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6fcc4047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_w(initialise):\n",
    "    if(initialise == 'random'):\n",
    "        w = np.random.randn(d,1)\n",
    "        print(\"w is initialised from N[0,1]\")\n",
    "    elif(initialise == 'zeros'):\n",
    "        w = np.zeros((d,1))\n",
    "        print(\"w is initialised as a zero vector\")\n",
    "    else:\n",
    "        print(\"Method unknown\")\n",
    "    return w\n",
    "\n",
    "def compute_mu(X, w):\n",
    "    mu = expit(np.dot(X,w))\n",
    "    mu = mu.reshape(X.shape[0],1)\n",
    "    return mu\n",
    "\n",
    "def first_derivative(w):\n",
    "    mu = compute_mu(X, w)\n",
    "    epsilon = 1e-12\n",
    "\n",
    "    grad = np.matmul(np.transpose(X), (mu-Y)) + w.reshape(d,1)\n",
    "    grad = grad.squeeze()\n",
    "    return(grad)\n",
    "\n",
    "def second_deivative(w,X,y):\n",
    "    mu = compute_mu(X, w)\n",
    "    R = np.eye(n)\n",
    "    for i in range(n):\n",
    "        R[i,i] = mu[i,0] * (1-mu[i,0])\n",
    "    return(np.dot(np.dot(np.transpose(X),R),X) + np.eye(d))\n",
    "\n",
    "def test(w, X, y):\n",
    "    n,d = X.shape\n",
    "    mu = compute_mu(X, w)\n",
    "    yhat = np.zeros((n,1)).astype(np.float64)\n",
    "    yhat[mu>0.5]=1\n",
    "    correct = np.sum(yhat==y)\n",
    "    return(correct,n)\n",
    "\n",
    "def train(initialise):\n",
    "\n",
    "    np.random.seed(0)\n",
    "    w = initialise_w(initialise)\n",
    "    for j in range(100):\n",
    "\n",
    "        grad1 = first_derivative(w.squeeze()).reshape(d,1)\n",
    "        H = second_deivative(w, X, Y)\n",
    "        delta_w = np.dot(np.linalg.inv(H),grad1)\n",
    "        w = w - delta_w\n",
    "        diff = np.linalg.norm(delta_w)\n",
    "\n",
    "        correct,n = test(w, Xtest, Ytest)\n",
    "        print(\"Iteration : {} \\t Accuracy : {}%\".format(j,correct/n*100))\n",
    "\n",
    "        if(diff < 1e-5):\n",
    "            print(\"tolerance reached at the iteration : \",j)\n",
    "            break\n",
    "    print(\"Training done...\")\n",
    "    print(\"Model weights : \", np.transpose(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f7142053",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w is initialised from N[0,1]\n",
      "Iteration : 0 \t Accuracy : 76.0%\n",
      "Iteration : 1 \t Accuracy : 34.8%\n",
      "Iteration : 2 \t Accuracy : 81.2%\n",
      "Iteration : 3 \t Accuracy : 55.800000000000004%\n",
      "Iteration : 4 \t Accuracy : 47.199999999999996%\n",
      "Iteration : 5 \t Accuracy : 63.800000000000004%\n",
      "Iteration : 6 \t Accuracy : 49.0%\n",
      "Iteration : 7 \t Accuracy : 63.6%\n",
      "Iteration : 8 \t Accuracy : 49.0%\n",
      "Iteration : 9 \t Accuracy : 63.6%\n",
      "Iteration : 10 \t Accuracy : 49.0%\n",
      "Iteration : 11 \t Accuracy : 63.6%\n",
      "Iteration : 12 \t Accuracy : 49.0%\n",
      "Iteration : 13 \t Accuracy : 63.6%\n",
      "Iteration : 14 \t Accuracy : 49.0%\n",
      "Iteration : 15 \t Accuracy : 63.6%\n",
      "Iteration : 16 \t Accuracy : 49.0%\n",
      "Iteration : 17 \t Accuracy : 63.6%\n",
      "Iteration : 18 \t Accuracy : 49.0%\n",
      "Iteration : 19 \t Accuracy : 63.6%\n",
      "Iteration : 20 \t Accuracy : 49.0%\n",
      "Iteration : 21 \t Accuracy : 63.6%\n",
      "Iteration : 22 \t Accuracy : 49.0%\n",
      "Iteration : 23 \t Accuracy : 63.6%\n",
      "Iteration : 24 \t Accuracy : 49.0%\n",
      "Iteration : 25 \t Accuracy : 63.6%\n",
      "Iteration : 26 \t Accuracy : 49.0%\n",
      "Iteration : 27 \t Accuracy : 63.6%\n",
      "Iteration : 28 \t Accuracy : 49.0%\n",
      "Iteration : 29 \t Accuracy : 63.6%\n",
      "Iteration : 30 \t Accuracy : 49.0%\n",
      "Iteration : 31 \t Accuracy : 63.6%\n",
      "Iteration : 32 \t Accuracy : 49.0%\n",
      "Iteration : 33 \t Accuracy : 63.6%\n",
      "Iteration : 34 \t Accuracy : 49.0%\n",
      "Iteration : 35 \t Accuracy : 63.6%\n",
      "Iteration : 36 \t Accuracy : 49.0%\n",
      "Iteration : 37 \t Accuracy : 63.6%\n",
      "Iteration : 38 \t Accuracy : 49.0%\n",
      "Iteration : 39 \t Accuracy : 63.6%\n",
      "Iteration : 40 \t Accuracy : 49.0%\n",
      "Iteration : 41 \t Accuracy : 63.6%\n",
      "Iteration : 42 \t Accuracy : 49.0%\n",
      "Iteration : 43 \t Accuracy : 63.6%\n",
      "Iteration : 44 \t Accuracy : 49.0%\n",
      "Iteration : 45 \t Accuracy : 63.6%\n",
      "Iteration : 46 \t Accuracy : 49.0%\n",
      "Iteration : 47 \t Accuracy : 63.6%\n",
      "Iteration : 48 \t Accuracy : 49.0%\n",
      "Iteration : 49 \t Accuracy : 63.6%\n",
      "Iteration : 50 \t Accuracy : 49.0%\n",
      "Iteration : 51 \t Accuracy : 63.6%\n",
      "Iteration : 52 \t Accuracy : 49.0%\n",
      "Iteration : 53 \t Accuracy : 63.6%\n",
      "Iteration : 54 \t Accuracy : 49.0%\n",
      "Iteration : 55 \t Accuracy : 63.6%\n",
      "Iteration : 56 \t Accuracy : 49.0%\n",
      "Iteration : 57 \t Accuracy : 63.6%\n",
      "Iteration : 58 \t Accuracy : 49.0%\n",
      "Iteration : 59 \t Accuracy : 63.6%\n",
      "Iteration : 60 \t Accuracy : 49.0%\n",
      "Iteration : 61 \t Accuracy : 63.6%\n",
      "Iteration : 62 \t Accuracy : 49.0%\n",
      "Iteration : 63 \t Accuracy : 63.6%\n",
      "Iteration : 64 \t Accuracy : 49.0%\n",
      "Iteration : 65 \t Accuracy : 63.6%\n",
      "Iteration : 66 \t Accuracy : 49.0%\n",
      "Iteration : 67 \t Accuracy : 63.6%\n",
      "Iteration : 68 \t Accuracy : 49.0%\n",
      "Iteration : 69 \t Accuracy : 63.6%\n",
      "Iteration : 70 \t Accuracy : 49.0%\n",
      "Iteration : 71 \t Accuracy : 63.6%\n",
      "Iteration : 72 \t Accuracy : 49.0%\n",
      "Iteration : 73 \t Accuracy : 63.6%\n",
      "Iteration : 74 \t Accuracy : 49.0%\n",
      "Iteration : 75 \t Accuracy : 63.6%\n",
      "Iteration : 76 \t Accuracy : 49.0%\n",
      "Iteration : 77 \t Accuracy : 63.6%\n",
      "Iteration : 78 \t Accuracy : 49.0%\n",
      "Iteration : 79 \t Accuracy : 63.6%\n",
      "Iteration : 80 \t Accuracy : 49.0%\n",
      "Iteration : 81 \t Accuracy : 63.6%\n",
      "Iteration : 82 \t Accuracy : 49.0%\n",
      "Iteration : 83 \t Accuracy : 63.6%\n",
      "Iteration : 84 \t Accuracy : 49.0%\n",
      "Iteration : 85 \t Accuracy : 63.6%\n",
      "Iteration : 86 \t Accuracy : 49.0%\n",
      "Iteration : 87 \t Accuracy : 63.6%\n",
      "Iteration : 88 \t Accuracy : 49.0%\n",
      "Iteration : 89 \t Accuracy : 63.6%\n",
      "Iteration : 90 \t Accuracy : 49.0%\n",
      "Iteration : 91 \t Accuracy : 63.6%\n",
      "Iteration : 92 \t Accuracy : 49.0%\n",
      "Iteration : 93 \t Accuracy : 63.6%\n",
      "Iteration : 94 \t Accuracy : 49.0%\n",
      "Iteration : 95 \t Accuracy : 63.6%\n",
      "Iteration : 96 \t Accuracy : 49.0%\n",
      "Iteration : 97 \t Accuracy : 63.6%\n",
      "Iteration : 98 \t Accuracy : 49.0%\n",
      "Iteration : 99 \t Accuracy : 63.6%\n",
      "Training done...\n",
      "Model weights :  [[ -976.93770373 -3299.21784144  1432.59566538   612.89583119]]\n"
     ]
    }
   ],
   "source": [
    "n,d = X.shape\n",
    "n1,d1 = Xtest.shape\n",
    "\n",
    "Y = Y.reshape(n,1)\n",
    "Ytest = Ytest.reshape(n1,1)\n",
    "\n",
    "train('random')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14cc15",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "042a88a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w is initialised from N[0,1]\n",
      "neg_log_posterior = 8737.0582 \tlog_like = -8734.3740 \tw_norm = 5.3684\n",
      "neg_log_posterior = 5647.9542 \tlog_like = -5646.5795 \tw_norm = 2.7493\n",
      "neg_log_posterior = 1804.1227 \tlog_like = -1803.4229 \tw_norm = 1.3996\n",
      "neg_log_posterior = 3582.6792 \tlog_like = -3578.6622 \tw_norm = 8.0339\n",
      "neg_log_posterior = 1804.1227 \tlog_like = -1803.4229 \tw_norm = 1.3996\n",
      "neg_log_posterior = 3582.6792 \tlog_like = -3578.6622 \tw_norm = 8.0339\n",
      "neg_log_posterior = 758.3798 \tlog_like = -757.7185 \tw_norm = 1.3227\n",
      "neg_log_posterior = 397.1981 \tlog_like = -396.7553 \tw_norm = 0.8856\n",
      "neg_log_posterior = 6140.0907 \tlog_like = -6117.5728 \tw_norm = 45.0357\n",
      "neg_log_posterior = 1268.5175 \tlog_like = -1267.4807 \tw_norm = 2.0736\n",
      "neg_log_posterior = 299.4804 \tlog_like = -299.2591 \tw_norm = 0.4426\n",
      "neg_log_posterior = 172.2659 \tlog_like = -172.0000 \tw_norm = 0.5317\n",
      "neg_log_posterior = 132.7590 \tlog_like = -132.4435 \tw_norm = 0.6311\n",
      "neg_log_posterior = 116.2467 \tlog_like = -115.8640 \tw_norm = 0.7654\n",
      "neg_log_posterior = 110.4690 \tlog_like = -110.0102 \tw_norm = 0.9176\n",
      "neg_log_posterior = 105.0568 \tlog_like = -104.4736 \tw_norm = 1.1664\n",
      "neg_log_posterior = 98.7290 \tlog_like = -97.9240 \tw_norm = 1.6099\n",
      "neg_log_posterior = 92.5931 \tlog_like = -91.4428 \tw_norm = 2.3005\n",
      "neg_log_posterior = 88.3392 \tlog_like = -86.7714 \tw_norm = 3.1355\n",
      "neg_log_posterior = 86.1883 \tlog_like = -84.2536 \tw_norm = 3.8695\n",
      "neg_log_posterior = 85.2712 \tlog_like = -83.1168 \tw_norm = 4.3089\n",
      "neg_log_posterior = 84.8208 \tlog_like = -82.5897 \tw_norm = 4.4622\n",
      "neg_log_posterior = 84.5950 \tlog_like = -82.3932 \tw_norm = 4.4036\n",
      "neg_log_posterior = 84.5423 \tlog_like = -82.4011 \tw_norm = 4.2824\n",
      "neg_log_posterior = 84.5323 \tlog_like = -82.4222 \tw_norm = 4.2203\n",
      "neg_log_posterior = 84.5294 \tlog_like = -82.4285 \tw_norm = 4.2018\n",
      "neg_log_posterior = 84.5283 \tlog_like = -82.4247 \tw_norm = 4.2073\n",
      "neg_log_posterior = 84.5277 \tlog_like = -82.4161 \tw_norm = 4.2232\n",
      "neg_log_posterior = 84.5276 \tlog_like = -82.4120 \tw_norm = 4.2311\n",
      "neg_log_posterior = 84.5276 \tlog_like = -82.4111 \tw_norm = 4.2330\n",
      "neg_log_posterior = 84.5276 \tlog_like = -82.4110 \tw_norm = 4.2332\n",
      "neg_log_posterior = 84.5276 \tlog_like = -82.4110 \tw_norm = 4.2332\n",
      "neg_log_posterior = 84.5276 \tlog_like = -82.4110 \tw_norm = 4.2332\n",
      "neg_log_posterior = 84.5276 \tlog_like = -82.4110 \tw_norm = 4.2332\n",
      "neg_log_posterior = 84.5276 \tlog_like = -82.4110 \tw_norm = 4.2332\n",
      "\n",
      "_____________Model trained______________\n",
      "\n",
      "\n",
      "Model weights :  [-1.50015252 -0.90812559 -0.98061604 -0.44317884]\n",
      "\n",
      "_____________Test Accuracy______________\n",
      "\n",
      "Accuracy : 95.6% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13078/1007993727.py:13: RuntimeWarning: overflow encountered in true_divide\n",
      "  return(np.sum((- Y*(1/(mu)) + (1-Y)*(1/(1+epsilon-mu)))*grad_mu,0) + w).squeeze()\n"
     ]
    }
   ],
   "source": [
    "# LBFGS\n",
    "def compute_mu(X, w):\n",
    "    phi=np.dot(X,w)\n",
    "    mu = norm.cdf(phi)\n",
    "    mu = mu.reshape(X.shape[0],1)\n",
    "    return mu\n",
    "\n",
    "def first_derivative(w):\n",
    "    mu = compute_mu(X, w)\n",
    "    epsilon = 1e-12\n",
    "    phi=np.dot(X,w)\n",
    "    grad_mu = X*(scipy.stats.norm.pdf(phi,0,1).reshape(-1,1))\n",
    "    return(np.sum((- Y*(1/(mu)) + (1-Y)*(1/(1+epsilon-mu)))*grad_mu,0) + w).squeeze()\n",
    "\n",
    "def second_deivative(w,X,y):\n",
    "    mu = compute_mu(X, w)\n",
    "    R = np.eye(n)\n",
    "\n",
    "    phi=np.dot(X,w)\n",
    "    for i in range(n):\n",
    "        t1 = (y[i] - mu[i,0])/(mu[i,0] * (1-mu[i,0]))\n",
    "        t2 = scipy.stats.norm.pdf(phi[i,0],0,1)\n",
    "        t3 = (1-y[i])/np.power(1-mu[i,0],2) + y[i]/np.power(mu[i,0],2)\n",
    "        R[i,i] = t1*t2*np.dot(X[i],w) + t3*t2*t2\n",
    "\n",
    "    return(np.dot(np.dot(np.transpose(X),R),X) + np.eye(d))\n",
    "\n",
    "def neg_log_posterior(w):\n",
    "    w=w.reshape(-1,1)\n",
    "    epsilon = 1e-12\n",
    "    mu = compute_mu(X, w)\n",
    "    prob_1 = Y*np.log(mu+epsilon)\n",
    "    prob_0 = (1-Y)*np.log(1-mu+epsilon)\n",
    "    log_like = np.sum(prob_1) + np.sum(prob_0)\n",
    "    w_norm = np.power(np.linalg.norm(w),2)\n",
    "    neg_log_pos = -log_like+w_norm/2\n",
    "    print(\"neg_log_posterior = {:.4f} \\tlog_like = {:.4f} \\tw_norm = {:.4f}\".format(neg_log_pos, log_like, w_norm))\n",
    "    return(neg_log_pos)\n",
    "\n",
    "def test(w, X, y):\n",
    "    n,d = X.shape\n",
    "    mu = compute_mu(X, w)\n",
    "    #print(mu.shape, n, d)\n",
    "    yhat = np.zeros((n,1)).astype(np.float64)\n",
    "    yhat[mu>0.5]=1\n",
    "    correct = np.sum(yhat==y)\n",
    "    return(correct,n)\n",
    "\n",
    "res = minimize(neg_log_posterior, initialise_w('random'), method='BFGS', jac=first_derivative,\n",
    "               tol= 1e-5, options={'maxiter': 100})\n",
    "correct,n = test(res.x, Xtest, Ytest)\n",
    "print(\"\\n_____________Model trained______________\\n\")\n",
    "print(\"\\nModel weights : \", res.x)\n",
    "print(\"\\n_____________Test Accuracy______________\\n\")\n",
    "\n",
    "print(\"Accuracy : {}% \".format(correct/n*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a827f864",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "76a40531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mu(X, w):\n",
    "    phi=np.dot(X,w)\n",
    "    mu = norm.cdf(phi)\n",
    "    mu = mu.reshape(X.shape[0],1)\n",
    "    return mu\n",
    "\n",
    "def first_derivative(w):\n",
    "    mu = compute_mu(X, w)\n",
    "    epsilon = 1e-12\n",
    "    phi=np.dot(X,w)\n",
    "    grad_mu = X*(scipy.stats.norm.pdf(phi,0,1).reshape(-1,1))\n",
    "    return(np.sum((- Y*(1/(mu)) + (1-Y)*(1/(1+epsilon-mu)))*grad_mu,0) + w).squeeze()\n",
    "\n",
    "def second_deivative(w,X,y):\n",
    "    mu = compute_mu(X, w)\n",
    "    R = np.eye(n)\n",
    "\n",
    "    phi=np.dot(X,w)\n",
    "    for i in range(n):\n",
    "        t1 = (y[i] - mu[i,0])/(mu[i,0] * (1-mu[i,0]))\n",
    "        t2 = scipy.stats.norm.pdf(phi[i,0],0,1)\n",
    "        t3 = (1-y[i])/np.power(1-mu[i,0],2) + y[i]/np.power(mu[i,0],2)\n",
    "        R[i,i] = t1*t2*np.dot(X[i],w) + t3*t2*t2\n",
    "\n",
    "    return(np.dot(np.dot(np.transpose(X),R),X) + np.eye(d))\n",
    "\n",
    "def neg_log_posterior(w):\n",
    "    w=w.reshape(-1,1)\n",
    "    epsilon = 1e-12\n",
    "    mu = compute_mu(X, w)\n",
    "    prob_1 = Y*np.log(mu+epsilon)\n",
    "    prob_0 = (1-Y)*np.log(1-mu+epsilon)\n",
    "    log_like = np.sum(prob_1) + np.sum(prob_0)\n",
    "    w_norm = np.power(np.linalg.norm(w),2)\n",
    "    neg_log_pos = -log_like+w_norm/2\n",
    "    print(\"neg_log_posterior = {:.4f} \\tlog_like = {:.4f} \\tw_norm = {:.4f}\".format(neg_log_pos, log_like, w_norm))\n",
    "    return(neg_log_pos)\n",
    "\n",
    "def test(w, X, y):\n",
    "    n,d = X.shape\n",
    "    mu = compute_mu(X, w)\n",
    "    #print(mu.shape, n, d)\n",
    "    yhat = np.zeros((n,1)).astype(np.float64)\n",
    "    yhat[mu>0.5]=1\n",
    "    correct = np.sum(yhat==y)\n",
    "    return(correct,n)\n",
    "\n",
    "def train(initialise):\n",
    "\n",
    "    np.random.seed(0)\n",
    "    w = initialise_w(initialise)\n",
    "    for j in range(100):\n",
    "\n",
    "        grad1 = first_derivative(w.squeeze()).reshape(d,1)\n",
    "        H = second_deivative(w, X, Y)\n",
    "        delta_w = np.dot(np.linalg.inv(H),grad1)\n",
    "        w = w - delta_w\n",
    "        diff = np.linalg.norm(delta_w)\n",
    "\n",
    "        correct,n = test(w, Xtest, Ytest)\n",
    "        print(\"Iteration : {} \\t Accuracy : {}%\".format(j,correct/n*100))\n",
    "\n",
    "        if(diff < 1e-5):\n",
    "            print(\"tolerance reached at the iteration : \",j)\n",
    "            break\n",
    "    print(\"Training done...\")\n",
    "    print(\"Model weights : \", np.transpose(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6c2973b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w is initialised as a zero vector\n",
      "Iteration : 0 \t Accuracy : 93.8%\n",
      "Iteration : 1 \t Accuracy : 94.8%\n",
      "Iteration : 2 \t Accuracy : 94.8%\n",
      "Iteration : 3 \t Accuracy : 95.0%\n",
      "Iteration : 4 \t Accuracy : 95.39999999999999%\n",
      "Iteration : 5 \t Accuracy : 95.6%\n",
      "Iteration : 6 \t Accuracy : 95.6%\n",
      "Iteration : 7 \t Accuracy : 95.6%\n",
      "Iteration : 8 \t Accuracy : 95.6%\n",
      "Iteration : 9 \t Accuracy : 95.6%\n",
      "tolerance reached at the iteration :  9\n",
      "Training done...\n",
      "Model weights :  [[-1.50015252 -0.90812559 -0.98061604 -0.44317884]]\n"
     ]
    }
   ],
   "source": [
    "n,d = X.shape\n",
    "n1,d1 = Xtest.shape\n",
    "\n",
    "Y = Y.reshape(n,1)\n",
    "Ytest = Ytest.reshape(n1,1)\n",
    "\n",
    "train('zeros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e8b32a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
